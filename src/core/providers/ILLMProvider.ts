import { TokenUsage, LLMConfig, BatchMetadata, BatchStatus } from "../types";

/**
 * Response from an LLM provider
 */
export interface LLMResponse {
    content: string;
    usage?: TokenUsage;
    raw?: any; // Provider-specific raw response
    thinking?: string; // For Anthropic extended thinking
}

/**
 * Provider-level batch request (prompt already generated by the node)
 */
export interface ProviderBatchRequest {
    customId: string;
    prompt: string;
}

/**
 * Raw per-item result from a provider batch (before node parsing)
 */
export interface ProviderBatchItemResult {
    customId: string;
    status: 'success' | 'failed' | 'expired' | 'cancelled';
    content?: string;       // Raw text from LLM
    error?: string;
    tokenUsage?: TokenUsage;
}

/**
 * Provider-level batch response
 */
export interface ProviderBatchResponse {
    status: BatchStatus;
    results?: ProviderBatchItemResult[];
    requestCounts?: {
        total: number;
        completed: number;
        failed: number;
        expired?: number;
        cancelled?: number;
    };
}

/**
 * Interface for LLM providers
 */
export interface ILLMProvider {
    /**
     * Core invocation method for sending prompts to the LLM
     * @param prompt The prompt text to send
     * @param config The LLM configuration
     * @returns The LLM response with content and usage data
     */
    invoke(prompt: string, config: LLMConfig): Promise<LLMResponse>;

    /**
     * Provider identifier
     */
    readonly provider: string;

    /**
     * Whether this provider supports batch processing
     */
    supportsBatch?(): boolean;

    /**
     * Create a batch of requests for async processing
     * @param requests Array of batch requests with prompts already generated
     * @param config The LLM configuration
     * @returns Serializable batch metadata for later retrieval
     */
    createBatch?(requests: ProviderBatchRequest[], config: LLMConfig): Promise<BatchMetadata>;

    /**
     * Retrieve batch results
     * @param metadata The batch metadata from createBatch
     * @param config The LLM configuration
     * @returns The batch response with status and results
     */
    retrieveBatch?(metadata: BatchMetadata, config: LLMConfig): Promise<ProviderBatchResponse>;
}
